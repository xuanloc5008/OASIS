# OASIS-Net: Adversarial Semi-supervised Network for Cervical and Fetal Ultrasound Imaging

OASIS-Net is a deep learning framework for medical image segmentation using semi-supervised learning with adversarial training techniques. The network combines Gradient Adversarial Perturbation (GAP) with Iterative Fast Gradient Sign Method (I-FGSM) to improve segmentation performance on ultrasound medical imaging.

## ğŸ¯ Overview

This repository implements a semi-supervised semantic segmentation approach specifically designed for cervical and fetal ultrasound imaging. The framework leverages both labeled and unlabeled data to train robust segmentation models while addressing the challenge of limited labeled medical data.

### Key Features

- **Adversarial Training**: Combines I-FGSM (Iterative Fast Gradient Sign Method) with GAP (Gradient Adversarial Perturbation) for robust pseudo-label generation
- **Semi-Supervised Learning**: Efficiently utilizes both labeled and unlabeled data with confidence-based pseudo-labeling
- **Strong-Weak Augmentation**: Implements sophisticated data augmentation strategies for unlabeled data
- **DeepLabV3Plus Architecture**: Uses DeepLabV3
- **K-Fold Cross Validation**: Supports 5-fold cross-validation for robust model evaluation
- **Comprehensive Metrics**: Tracks Dice Score, IoU, and Hausdorff Distance using MONAI

## ğŸš€ Installation

### Requirements

- Python 3.8+
- CUDA 11.x or higher (for GPU support)
- PyTorch 1.10+

### Setup

1. Clone the repository:
```bash
git clone https://github.com/john-minhle/OASIS.git
cd OASIS
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Key Dependencies

- PyTorch & torchvision
- MONAI (Medical Open Network for AI)
- Albumentations (data augmentation)
- OpenCV
- NumPy, Pandas
- PyYAML
- tqdm

## ğŸ“ Project Structure

```
OASIS/
â”œâ”€â”€ OASIS.py                    # Main training script
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ Configs/
â”‚   â””â”€â”€ multi_train_local.yml   # Training configuration
â”œâ”€â”€ Datasets/
â”‚   â”œâ”€â”€ create_dataset.py       # Dataset creation and loading
â”‚   â”œâ”€â”€ transform.py            # Data transformations
â”‚   â”œâ”€â”€ data_augmentation.py    # Augmentation strategies
â”‚   â””â”€â”€ unimatch_utils.py       # Utility functions
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ decoder.py              # Decoder architecture
â”‚   â””â”€â”€ DeepLabV3Plus/          # DeepLabV3+ implementation
â”‚       â”œâ”€â”€ modeling.py         # Model definitions
â”‚       â”œâ”€â”€ _deeplab.py         # DeepLab components
â”‚       â”œâ”€â”€ utils.py            # Model utilities
â”‚       â””â”€â”€ backbone/           # Backbone networks
â””â”€â”€ Utils/
    â”œâ”€â”€ utils.py                # General utilities
    â””â”€â”€ thresh_helper.py        # Threshold helpers
```

## ğŸ“Š Dataset Preparation

### Data Format

The dataset should be organized as follows:

```
data_processed/
â”œâ”€â”€ fhps/                       # Dataset name
â”‚   â”œâ”€â”€ images/                 # Input images (.npy format)
â”‚   â”‚   â”œâ”€â”€ image_001.npy
â”‚   â”‚   â”œâ”€â”€ image_002.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ labels/                 # Ground truth labels (.npy format)
â”‚   â”‚   â”œâ”€â”€ image_001.npy
â”‚   â”‚   â”œâ”€â”€ image_002.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ fold_label_1.txt        # Labeled data for fold 1
â”‚   â”œâ”€â”€ fold_label_2.txt        # Labeled data for fold 2
â”‚   â”œâ”€â”€ fold_label_3.txt        # Labeled data for fold 3
â”‚   â”œâ”€â”€ fold_label_4.txt        # Labeled data for fold 4
â”‚   â”œâ”€â”€ fold_label_5.txt        # Labeled data for fold 5
â”‚   â””â”€â”€ unlabeled.txt           # Unlabeled data filenames
```

### Data Format Specifications

- **Images**: NumPy arrays (.npy) with shape `(H, W, 3)` or `(H, W)` for grayscale
- **Labels**: NumPy arrays (.npy) with shape `(H, W)` containing class indices (0, 1, 2, ...)
- **Text Files**: Plain text files with one filename per line (e.g., `image_001.npy`)

### Key Parameters

- **conf_thresh**: Confidence threshold for accepting pseudo-labels (default: 0.95)
- **img_size**: Input image size (will be resized to this dimension)
- **l_batchsize**: Batch size for labeled data
- **u_batchsize**: Batch size for unlabeled data
- **num_epochs**: Total training epochs
- **num_classes**: Number of segmentation classes (background + foreground classes)

## ğŸ‹ï¸ Training

### Basic Training

Train the model with default settings:

```bash
python OASIS.py --config_yml Configs/multi_train_local.yml --exp experiment_name
```

### Training Process

The training script will:
1. Load labeled and unlabeled data based on fold splits
2. Initialize DeepLabV3+ model with pretrained ResNet101 backbone
3. Train using combined supervised and unsupervised losses
4. Validate after each epoch
5. Save the best model based on validation Dice score
6. Generate logs and results in the experiment directory

### Output Structure

```
checkpoints/fhps_test/my_experiment/
â”œâ”€â”€ fold1/
â”‚   â”œâ”€â”€ best.pth                # Best model weights
â”‚   â”œâ”€â”€ exp_config.yml          # Configuration used
â”‚   â”œâ”€â”€ log.txt                 # Training logs
â”‚   â””â”€â”€ test_results.txt        # Test metrics
â”œâ”€â”€ fold2/
â”‚   â””â”€â”€ ...
â””â”€â”€ fold5/
    â””â”€â”€ ...
```

## ğŸ“ˆ Evaluation

The model is automatically evaluated during training. Test results include:

- **Dice Score**: Overlap between prediction and ground truth
- **IoU (Intersection over Union)**: Jaccard index
- **Hausdorff Distance**: Maximum distance between boundaries
- **Loss**: Validation loss

Results are saved in `test_results.txt` for each fold.

## How to run the app

- The app is dockerized, so if you want to run it, download the pre-trained model and place it in the same diretory level with the ```app.py``` file
- Then run this command ```bash docker-compose up --build```
- After that, get access to the link http://172.21.0.3:3000